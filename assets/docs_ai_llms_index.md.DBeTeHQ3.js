import{_ as e,c as o,o as a,a1 as r}from"./chunks/framework.DDNAyvCW.js";const u=JSON.parse('{"title":"Model List","description":"","frontmatter":{},"headers":[],"relativePath":"docs/ai/llms/index.md","filePath":"docs/ai/llms/index.md"}'),n={name:"docs/ai/llms/index.md"};function i(d,t,s,l,g,h){return a(),o("div",null,t[0]||(t[0]=[r('<h1 id="model-list" tabindex="-1">Model List <a class="header-anchor" href="#model-list" aria-label="Permalink to &quot;Model List&quot;">​</a></h1><p>Chaterm supports multiple model providers, offering a flexible AI programming experience. From built-in models to custom integrations, meeting the needs of different scenarios.</p><h2 id="built-in-models" tabindex="-1">Built-in Models <a class="header-anchor" href="#built-in-models" aria-label="Permalink to &quot;Built-in Models&quot;">​</a></h2><p>Chaterm includes multiple high-quality code models out of the box, requiring no additional configuration:</p><h3 id="chain-of-thought-models" tabindex="-1">Chain-of-Thought Models <a class="header-anchor" href="#chain-of-thought-models" aria-label="Permalink to &quot;Chain-of-Thought Models&quot;">​</a></h3><p>These models have deep reasoning capabilities, able to analyze problems step by step and provide detailed solutions:</p><table tabindex="0"><thead><tr><th>Model</th><th>Features</th><th>Use Cases</th><th>Reasoning Ability</th></tr></thead><tbody><tr><td><strong>DeepSeek-R1 (thinking)</strong></td><td>Advanced model with deep reasoning</td><td>Complex algorithm design, architecture analysis</td><td>High</td></tr><tr><td><strong>GLM-4.6 (thinking)</strong></td><td>Strong logical reasoning ability</td><td>Code review, problem diagnosis</td><td>Medium-High</td></tr><tr><td><strong>Qwen-Plus (thinking)</strong></td><td>Alibaba Cloud Qwen chain-of-thought model</td><td>Multi-language development, cross-platform projects</td><td>Medium-High</td></tr></tbody></table><h3 id="standard-models" tabindex="-1">Standard Models <a class="header-anchor" href="#standard-models" aria-label="Permalink to &quot;Standard Models&quot;">​</a></h3><p>Fast-responding standard models, suitable for daily programming tasks:</p><table tabindex="0"><thead><tr><th>Model</th><th>Features</th><th>Use Cases</th><th>Response Speed</th></tr></thead><tbody><tr><td><strong>DeepSeek-V3.2</strong></td><td>Supports complex code analysis</td><td>Large project refactoring, performance optimization</td><td>Fast</td></tr><tr><td><strong>Qwen-Plus</strong></td><td>High-performance code generation model</td><td>Enterprise application development</td><td>Fast</td></tr><tr><td><strong>GLM-4.6</strong></td><td>Excellent code generation capability</td><td>Rapid prototyping, feature implementation</td><td>Medium</td></tr><tr><td><strong>Qwen-Turbo</strong></td><td>Fast-responding lightweight model</td><td>Real-time programming assistance, rapid iteration</td><td>Very Fast</td></tr></tbody></table><h2 id="adding-custom-models" tabindex="-1">Adding Custom Models <a class="header-anchor" href="#adding-custom-models" aria-label="Permalink to &quot;Adding Custom Models&quot;">​</a></h2><p>You can add more model providers in settings to extend Chaterm&#39;s functionality. Supports multiple integration methods to meet different needs:</p><h3 id="model-integration" tabindex="-1">Model Integration <a class="header-anchor" href="#model-integration" aria-label="Permalink to &quot;Model Integration&quot;">​</a></h3><h4 id="_1-litellm-integration" tabindex="-1">1. LiteLLM Integration <a class="header-anchor" href="#_1-litellm-integration" aria-label="Permalink to &quot;1. LiteLLM Integration&quot;">​</a></h4><p>Connect to multiple model providers through LiteLLM, supporting unified API interface:</p><table tabindex="0"><thead><tr><th>Configuration Item</th><th>Description</th><th>Required</th></tr></thead><tbody><tr><td><strong>URL Address</strong></td><td>LiteLLM service endpoint</td><td>Required</td></tr><tr><td><strong>API Key</strong></td><td>Access key</td><td>Required</td></tr><tr><td><strong>Model Name</strong></td><td>Specific model to use</td><td>Required</td></tr></tbody></table><p><strong>Advantages:</strong> Unified interface, supports multiple model providers</p><h4 id="_2-openai-integration" tabindex="-1">2. OpenAI Integration <a class="header-anchor" href="#_2-openai-integration" aria-label="Permalink to &quot;2. OpenAI Integration&quot;">​</a></h4><p>Directly connect to OpenAI service, using official API:</p><table tabindex="0"><thead><tr><th>Configuration Item</th><th>Description</th><th>Required</th></tr></thead><tbody><tr><td><strong>OpenAI URL Address</strong></td><td>OpenAI API endpoint</td><td>Required</td></tr><tr><td><strong>OpenAI API Key</strong></td><td>OpenAI access key</td><td>Required</td></tr><tr><td><strong>Model Name</strong></td><td>GPT-5, GPT-4, etc.</td><td>Required</td></tr></tbody></table><p><strong>Advantages:</strong> Official support, stable and reliable</p><h4 id="_3-amazon-bedrock" tabindex="-1">3. Amazon Bedrock <a class="header-anchor" href="#_3-amazon-bedrock" aria-label="Permalink to &quot;3. Amazon Bedrock&quot;">​</a></h4><p>Use AWS Bedrock service, enterprise-grade solution:</p><table tabindex="0"><thead><tr><th>Configuration Item</th><th>Description</th><th>Required</th></tr></thead><tbody><tr><td><strong>AWS Access Key</strong></td><td>AWS access key</td><td>Required</td></tr><tr><td><strong>AWS Secret Key</strong></td><td>AWS secret key</td><td>Required</td></tr><tr><td><strong>AWS Session Token</strong></td><td>Session token</td><td>Optional</td></tr><tr><td><strong>AWS Region</strong></td><td>Service region</td><td>Required</td></tr><tr><td><strong>Custom VPC Endpoint</strong></td><td>Private network endpoint</td><td>Optional</td></tr><tr><td><strong>Cross-Region Inference</strong></td><td>Multi-region deployment</td><td>Optional</td></tr><tr><td><strong>Model Name</strong></td><td>Bedrock model</td><td>Required</td></tr></tbody></table><p><strong>Advantages:</strong> Enterprise-grade security, high availability</p><h4 id="_4-deepseek-integration" tabindex="-1">4. DeepSeek Integration <a class="header-anchor" href="#_4-deepseek-integration" aria-label="Permalink to &quot;4. DeepSeek Integration&quot;">​</a></h4><p>Connect to DeepSeek official API, using advanced models:</p><table tabindex="0"><thead><tr><th>Configuration Item</th><th>Description</th><th>Required</th></tr></thead><tbody><tr><td><strong>DeepSeek API Key</strong></td><td>DeepSeek access key</td><td>Required</td></tr><tr><td><strong>Model Name</strong></td><td>DeepSeek model</td><td>Required</td></tr></tbody></table><p><strong>Advantages:</strong> Advanced models, strong reasoning capability</p><h3 id="local-model-deployment" tabindex="-1">Local Model Deployment <a class="header-anchor" href="#local-model-deployment" aria-label="Permalink to &quot;Local Model Deployment&quot;">​</a></h3><h4 id="_5-ollama-local-deployment" tabindex="-1">5. Ollama Local Deployment <a class="header-anchor" href="#_5-ollama-local-deployment" aria-label="Permalink to &quot;5. Ollama Local Deployment&quot;">​</a></h4><p>Use locally deployed Ollama models to protect data privacy:</p><table tabindex="0"><thead><tr><th>Configuration Item</th><th>Description</th><th>Required</th></tr></thead><tbody><tr><td><strong>Ollama URL Address</strong></td><td>Local Ollama service address</td><td>Required</td></tr><tr><td><strong>Model Name</strong></td><td>Local model name</td><td>Required</td></tr></tbody></table><p><strong>Advantages:</strong> Data privacy, offline available</p><h2 id="usage-instructions" tabindex="-1">Usage Instructions <a class="header-anchor" href="#usage-instructions" aria-label="Permalink to &quot;Usage Instructions&quot;">​</a></h2><h3 id="quick-start" tabindex="-1">Quick Start <a class="header-anchor" href="#quick-start" aria-label="Permalink to &quot;Quick Start&quot;">​</a></h3><ol><li><strong>Enter Settings Page</strong> - Click the settings icon in the top right corner</li><li><strong>Select &quot;Models&quot; Tab</strong> - Find model settings in the left menu</li><li><strong>Click &quot;Add Model&quot; Button</strong> - Start adding new model configuration</li><li><strong>Select Corresponding Provider</strong> - Choose appropriate model provider based on needs</li><li><strong>Fill in Required Configuration</strong> - Fill in configuration items according to the table requirements</li><li><strong>Save and Test Connection</strong> - Verify configuration is correct</li></ol><h3 id="configuration-tips" tabindex="-1">Configuration Tips <a class="header-anchor" href="#configuration-tips" aria-label="Permalink to &quot;Configuration Tips&quot;">​</a></h3><ul><li><strong>API Key Security</strong>: Use environment variables to store sensitive information</li><li><strong>Connection Testing</strong>: Be sure to test connection after configuration</li><li><strong>Model Switching</strong>: Can configure multiple models and switch as needed</li><li><strong>Performance Monitoring</strong>: Pay attention to model response time and usage costs</li></ul><h2 id="model-selection-recommendations" tabindex="-1">Model Selection Recommendations <a class="header-anchor" href="#model-selection-recommendations" aria-label="Permalink to &quot;Model Selection Recommendations&quot;">​</a></h2><h3 id="by-use-case" tabindex="-1">By Use Case <a class="header-anchor" href="#by-use-case" aria-label="Permalink to &quot;By Use Case&quot;">​</a></h3><table tabindex="0"><thead><tr><th>Use Case</th><th>Recommended Model</th><th>Reasons</th></tr></thead><tbody><tr><td><strong>Daily Programming</strong></td><td>Qwen-Turbo</td><td>Fast response, low cost</td></tr><tr><td><strong>Complex Tasks</strong></td><td>DeepSeek-R1 (thinking)</td><td>Strong reasoning, deep analysis</td></tr><tr><td><strong>Local Deployment</strong></td><td>Ollama</td><td>Data privacy, offline available</td></tr><tr><td><strong>Enterprise Applications</strong></td><td>Amazon Bedrock</td><td>Stable and reliable, secure and compliant</td></tr><tr><td><strong>Multi-language Development</strong></td><td>Qwen-Plus (thinking)</td><td>Multi-language support, strong understanding</td></tr><tr><td><strong>Rapid Prototyping</strong></td><td>GLM-4.6</td><td>Fast generation, suitable for iteration</td></tr></tbody></table><h3 id="by-performance-requirements" tabindex="-1">By Performance Requirements <a class="header-anchor" href="#by-performance-requirements" aria-label="Permalink to &quot;By Performance Requirements&quot;">​</a></h3><h4 id="pursuing-speed" tabindex="-1">Pursuing Speed <a class="header-anchor" href="#pursuing-speed" aria-label="Permalink to &quot;Pursuing Speed&quot;">​</a></h4><ul><li><strong>Qwen-Turbo</strong> - Fastest response</li><li><strong>GLM-4.6</strong> - Balanced performance and quality</li></ul><h4 id="pursuing-quality" tabindex="-1">Pursuing Quality <a class="header-anchor" href="#pursuing-quality" aria-label="Permalink to &quot;Pursuing Quality&quot;">​</a></h4><ul><li><strong>DeepSeek-R1 (thinking)</strong> - Strongest reasoning</li><li><strong>DeepSeek-V3.2</strong> - Complex analysis</li></ul><h4 id="pursuing-cost-efficiency" tabindex="-1">Pursuing Cost Efficiency <a class="header-anchor" href="#pursuing-cost-efficiency" aria-label="Permalink to &quot;Pursuing Cost Efficiency&quot;">​</a></h4><ul><li><strong>Qwen-Turbo</strong> - Lowest cost</li><li><strong>Ollama Local</strong> - No usage fees</li></ul><h4 id="pursuing-privacy" tabindex="-1">Pursuing Privacy <a class="header-anchor" href="#pursuing-privacy" aria-label="Permalink to &quot;Pursuing Privacy&quot;">​</a></h4><ul><li><strong>Ollama Local</strong> - Fully localized</li><li><strong>Amazon Bedrock</strong> - Enterprise-grade security</li></ul>',51)]))}const p=e(n,[["render",i]]);export{u as __pageData,p as default};
